# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WSr1lvQUfbKBnKT33CxOnmoBreuoEEyb
"""

from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

import os
os.environ["OPENAI_API_KEY"] = "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"

### model init
llm = llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7)

### assign memory for cov
memory = ConversationBufferMemory()

conversation = ConversationChain(llm = llm , memory = memory , verbose = True)

#### then run the chatbot

print("welcome !")

while True :
  user_input = input("you: ")
  if user_input.lower() in["exit, quit", "q", "exit()"]:
    print("Exiting the chatbot. Goodbye!")
    break
  respoance =conversation.predict(input =user_input)
  print("chatbot:" , respoance)
